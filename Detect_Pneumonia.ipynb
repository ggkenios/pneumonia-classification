{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phV4ysCbPolU"
   },
   "source": [
    "# &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Detect Pneumonia (2021)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5yNstt-Jimt"
   },
   "source": [
    "## The code will be split in 5 main parts: <br>\n",
    "\n",
    "[<li> 1. Libraries](#1) <br>\n",
    "\n",
    "[<li> 2. Input](#2) <br>\n",
    "&emsp;  &emsp;  &ensp; [*2.1 Hyperparameters*](#2.1) <br>\n",
    "&emsp;  &emsp;  &ensp; [ *2.2 Data Directories*](#2.2) <br>\n",
    "&emsp;  &emsp;  &ensp; [*2.3 Generalization Variables*](#2.3) <br>\n",
    "\n",
    "[<li> 3. Data Preprocessing](#3) <br>\n",
    "&emsp;  &emsp;  &ensp; [*3.1 Setting Variables*](#3.1) <br>\n",
    "&emsp;  &emsp;  &ensp; [*3.2 Extra Data*](#3.2) <br>\n",
    "&emsp;  &emsp;  &ensp; [*3.3 Image Read*](#3.3) <br>\n",
    "&emsp;  &emsp;  &ensp; [*3.4 Cross Validation Split*](#3.4) <br>\n",
    "&emsp;  &emsp;  &ensp; [*3.5 Data Preperation*](#3.5) <br>\n",
    "\n",
    "[<li> 4. Model ](#4) <br>\n",
    "&emsp;  &emsp;  &ensp; [*4.1 Callbacks*](#4.1) <br>\n",
    "&emsp;  &emsp;  &ensp; [*4.2 Models* ](#4.2) <br>\n",
    "&emsp;  &emsp;  &ensp; [*4.3 Model Fit*](#4.3) <br>\n",
    "&emsp;  &emsp;  &ensp; [*4.4 Fine Tuning*](#4.4) <br>\n",
    "\n",
    "[<li> 5. Predictions ](#5) <br>\n",
    "&emsp;  &emsp;  &ensp; [ *5.1 Test Images*](#5.1) <br>\n",
    "&emsp;  &emsp;  &ensp; [ *5.2 Export Predictions*](#5.2) <br>\n",
    "&emsp;  &emsp;  &ensp; [ *5.3 Voting Ensemble*](#5.3) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHi50zB1OLOe"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "##  1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70rdAZD9EK8m"
   },
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TF\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "# Keras\n",
    "from keras import optimizers, losses\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjKmQL3hQcyV"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2. Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40xE31gaTb2K"
   },
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### &ensp; *2.1 Hyperparameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPiKVoAAUcyg"
   },
   "outputs": [],
   "source": [
    "selected_model = 'efficientnet_B3'\n",
    "\n",
    "Hyperparameters = {\n",
    "    'efficientnet_B0':{\n",
    "        'img_size':224,\n",
    "        'epochs': 25,\n",
    "        'batch_size':8,\n",
    "        'learning_rate':1e-4,\n",
    "        'fine_tune_epochs':25,\n",
    "        'unfreeze_layers':-25\n",
    "        },\n",
    "    'efficientnet_B1':{\n",
    "        'img_size':240,\n",
    "        'epochs': 50,\n",
    "        'batch_size':16,\n",
    "        'learning_rate':1e-3,\n",
    "        'fine_tune_epochs':80,\n",
    "        'unfreeze_layers':-40\n",
    "        },\n",
    "    'efficientnet_B2':{\n",
    "        'img_size':300,\n",
    "        'epochs': 50,\n",
    "        'batch_size':32,\n",
    "        'learning_rate':1e-3,\n",
    "        'fine_tune_epochs':80,\n",
    "        'unfreeze_layers':-25\n",
    "        },\n",
    "    'efficientnet_B3':{\n",
    "        'img_size':300,\n",
    "        'epochs': 40,\n",
    "        'batch_size':32,\n",
    "        'learning_rate':1e-3,\n",
    "        'fine_tune_epochs':50,\n",
    "        'unfreeze_layers':-30\n",
    "        },\n",
    "    'efficientnet_B4':{\n",
    "        'img_size':300,\n",
    "        'epochs': 25,\n",
    "        'batch_size':8,\n",
    "        'learning_rate':1e-4,\n",
    "        'fine_tune_epochs':25,\n",
    "        'unfreeze_layers':-30\n",
    "        },\n",
    "    'efficientnet_B5':{\n",
    "        'img_size':456,\n",
    "        'epochs': 25,\n",
    "        'batch_size':8,\n",
    "        'learning_rate':1e-4,\n",
    "        'fine_tune_epochs':25,\n",
    "        'unfreeze_layers':-25\n",
    "        }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKnEHYNeShT_"
   },
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### &ensp; *2.2 Data Directories* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwh78AoyQn7F"
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "train_path = '/Users/thxsg/OneDrive - International Hellenic University/1. IHU (2021)/5. Advanced Machine Learning/2. Homework/detect-pneumonia-fall-2021/train_images'\n",
    "test_path = '/Users/thxsg/OneDrive - International Hellenic University/1. IHU (2021)/5. Advanced Machine Learning/2. Homework/detect-pneumonia-fall-2021/test_images'\n",
    "label_csv = '/Users/thxsg/OneDrive - International Hellenic University/1. IHU (2021)/5. Advanced Machine Learning/2. Homework/detect-pneumonia-fall-2021/labels_train.csv'\n",
    "\n",
    "# Checkpoints\n",
    "path_c = '/Users/thxsg/Documents/Python Scripts/Checkpoints/'\n",
    "\n",
    "# Submissions\n",
    "sub = '/Users/thxsg/Documents/Python Scripts/Submissions/'\n",
    "\n",
    "# Softmax output for the model\n",
    "soft_single = '/Users/thxsg/Documents/Python Scripts/Soft-Votes.csv'\n",
    "\n",
    "# Path to store softamax CSVs for Voting\n",
    "soft_path = '/Users/thxsg/OneDrive - International Hellenic University/1. IHU (2021)/5. Advanced Machine Learning/2. Homework/Attempts/Softmaxes'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbzduVtVg4L"
   },
   "source": [
    "<a name=\"2.3\"></a>\n",
    "### &ensp; *2.3 Generalization Variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmXG9NrMV5Cl"
   },
   "outputs": [],
   "source": [
    "colx = 'file_name'\n",
    "coly = 'class_id'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnOjI0o4WCao"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3. Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_liboCODSNoC"
   },
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### &ensp; *3.1 Setting Variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tujFAAsURwz7"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "img_size = Hyperparameters[selected_model]['img_size']\n",
    "epochs = Hyperparameters[selected_model]['epochs']\n",
    "batch_size = Hyperparameters[selected_model]['batch_size']\n",
    "learning_rate = Hyperparameters[selected_model]['learning_rate']\n",
    "fine_tune_epochs = Hyperparameters[selected_model]['fine_tune_epochs']\n",
    "unfreeze_layers = Hyperparameters[selected_model]['unfreeze_layers']\n",
    "\n",
    "# Other Variables\n",
    "labels = pd.read_csv(label_csv)\n",
    "classes = labels[coly].unique()\n",
    "num_cl = len(classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwYaiHdTHUMA"
   },
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### &ensp; *3.2 Extra Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lE5_oXXBHTsB"
   },
   "outputs": [],
   "source": [
    "# Round the max number of images of the biggest class to a multiplier of 10,\n",
    "# for cross validation.\n",
    "lengths = []\n",
    "for i in range(0, num_cl):\n",
    "   exec('lengths.append(len(labels.loc[labels[coly] == ' + str(i) + ']))')\n",
    "maxlen = max(lengths)\n",
    "rr = - maxlen%10\n",
    "new_max = maxlen + rr\n",
    "\n",
    "# Then calculate the numbers for each class needed for data augmentation,\n",
    "# so each class has the same number of images.\n",
    "for i in range(0, num_cl):\n",
    "   exec('d' + str(i) + ' = new_max - len(labels.loc[labels[coly] == ' + str(i) + '])')\n",
    "\n",
    "# Creating the list for the images that will be augmented.\n",
    "exec('df_extra = labels.loc[labels[coly] == (num_cl - 1)].iloc[:d' + str(num_cl - 1) + ',][colx]')\n",
    "for i in range(num_cl - 2, -1, -1):\n",
    "  exec('df_extra = labels.loc[labels[coly] == ' + str(i) + '].iloc[:d' + str(i) + ',][colx].append(df_extra)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b6PN-F9Hh3O"
   },
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### &ensp; *3.3 Image Read*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sk_44LnV8PXM"
   },
   "outputs": [],
   "source": [
    "# Read Images\n",
    "x_train = []\n",
    "for img in labels[colx]:\n",
    "  img_array = cv2.imread(os.path.join(train_path, img))\n",
    "  res_array = cv2.resize(img_array, (img_size, img_size))\n",
    "  x_train.append(res_array)\n",
    "\n",
    "x_train_extra = []\n",
    "for img in df_extra:\n",
    "  img_array = cv2.imread(os.path.join(train_path, img))\n",
    "  res_array = cv2.resize(img_array, (img_size, img_size))\n",
    "  x_train_extra.append(res_array)\n",
    "\n",
    "# Transform Data to Tensorflow shape\n",
    "x_train = np.array(x_train).reshape(-1, img_size, img_size, 3)\n",
    "x_train_extra = np.array(x_train_extra).reshape(-1, img_size, img_size, 3)\n",
    "\n",
    "# Create augmentation function for extra data\n",
    "augm = keras.Sequential([layers.experimental.preprocessing.RandomZoom(0.05),])\n",
    "\n",
    "# Image data\n",
    "x_train = np.concatenate((x_train, augm(x_train_extra)))\n",
    "\n",
    "# Image data labels\n",
    "y_train = np.array(labels[coly])\n",
    "for i in range(0, num_cl):\n",
    "  exec('y_train = np.append(y_train, np.repeat(' + str(i) + ', d' + str(i) + '))')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2wUIQBW-Db4"
   },
   "source": [
    "<a name=\"3.4\"></a>\n",
    "### &ensp; *3.4 Cross Validation Split*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLyaadTM6Rcn"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    stratify = y_train,\n",
    "    test_size = 0.1,\n",
    "    random_state = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfpDjOHqyrD7"
   },
   "source": [
    "<a name=\"3.5\"></a>\n",
    "### &ensp; *3.5 Data Preperation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_tzja6jRx1p"
   },
   "outputs": [],
   "source": [
    "# 1-hot-encoding, for categorical cross entropy\n",
    "y_train = to_categorical(y_train, num_cl)\n",
    "y_val = to_categorical(y_val, num_cl)\n",
    "\n",
    "# Prepare batches for the model\n",
    "train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).batch(batch_size)\n",
    "val = tf.data.Dataset.from_tensor_slices((x_val,y_val)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ogpUn1Wzwi9"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHEse4tT_BFW"
   },
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### &ensp; *4.1 Callbacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwLJHgvo24g2"
   },
   "outputs": [],
   "source": [
    "# Checkpoints\n",
    "checkpoint_acc = tf.keras.callbacks.ModelCheckpoint(filepath = path_c + 'Acc_CP - ' + selected_model + '.h5',\n",
    "                                                    monitor='val_acc',\n",
    "                                                    save_best_only=True)\n",
    "\n",
    "checkpoint_loss = tf.keras.callbacks.ModelCheckpoint(filepath = path_c + 'Loss_CP - ' + selected_model + '.h5',\n",
    "                                                     monitor='val_loss',\n",
    "                                                     save_best_only=True)\n",
    "\n",
    "checkpoint_acc_ft = tf.keras.callbacks.ModelCheckpoint(filepath = path_c + 'Acc_CP - ' + selected_model + ' FT.h5',\n",
    "                                                       monitor='val_acc',\n",
    "                                                       save_best_only=True)\n",
    "\n",
    "checkpoint_loss_ft = tf.keras.callbacks.ModelCheckpoint(filepath = path_c + 'Loss_CP - ' + selected_model + ' FT.h5',\n",
    "                                                        monitor='val_loss',\n",
    "                                                        save_best_only=True)\n",
    "\n",
    "# Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=13,\n",
    "                                                     monitor='val_acc',\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "# Learning Rate Modifiers\n",
    "def lr_schedule(epoch):\n",
    "    global epochs\n",
    "    global learning_rate\n",
    "\n",
    "    lr = learning_rate\n",
    "    if epoch > 0.9*epochs:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 0.8*epochs:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 0.6*epochs:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 0.4*epochs:\n",
    "        lr *= 1e-1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=np.sqrt(0.1),\n",
    "                               mode='min',\n",
    "                               patience=3,\n",
    "                               min_lr=0.5e-6,\n",
    "                               verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGQWxCn0zNoW"
   },
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### &ensp; *4.2 Models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xx8B8SkqeOlU"
   },
   "outputs": [],
   "source": [
    "def efficientnet_B0():\n",
    "        global num_cl\n",
    "        global img_size\n",
    "        \n",
    "        base_model = tf.keras.applications.EfficientNetB0(include_top=False,\n",
    "                                     weights='imagenet',\n",
    "                                     input_shape=(img_size, img_size, 3))\n",
    "        x = base_model.output\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(num_cl, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def efficientnet_B1():\n",
    "        global num_cl\n",
    "        global img_size\n",
    "\n",
    "        base_model = tf.keras.applications.EfficientNetB1(include_top=False,\n",
    "                                     weights='imagenet',\n",
    "                                     input_shape=(img_size, img_size, 3))\n",
    "        x = base_model.output\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(num_cl, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def efficientnet_B2():\n",
    "        global num_cl\n",
    "        global img_size\n",
    "\n",
    "        base_model = tf.keras.applications.EfficientNetB2(include_top=False,\n",
    "                                     weights='imagenet',\n",
    "                                     input_shape=(img_size, img_size, 3))\n",
    "        x = base_model.output\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(num_cl, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def efficientnet_B3():\n",
    "        global num_cl\n",
    "        global img_size\n",
    "\n",
    "        base_model = tf.keras.applications.EfficientNetB3(include_top=False,\n",
    "                                     weights='imagenet',\n",
    "                                     input_shape=(img_size, img_size, 3))\n",
    "        x = base_model.output\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(num_cl, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def efficientnet_B4():\n",
    "        global num_cl\n",
    "        global img_size\n",
    "\n",
    "        base_model = tf.keras.applications.EfficientNetB4(include_top=False,\n",
    "                                     weights='imagenet',\n",
    "                                     input_shape=(img_size, img_size, 3))\n",
    "        x = base_model.output\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(num_cl, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def efficientnet_B5():\n",
    "        global num_cl\n",
    "        global img_size\n",
    "\n",
    "        base_model = tf.keras.applications.EfficientNetB5(include_top=False,\n",
    "                                     weights='imagenet',\n",
    "                                     input_shape=(img_size, img_size, 3))\n",
    "        x = base_model.output\n",
    "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(512, activation=None, kernel_regularizer = L1L2(l1=1e-5, l2=1e-3))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('swish')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        out = Dense(num_cl, activation='softmax')(x)\n",
    "\n",
    "        model = Model(inputs=base_model.input, outputs=out)\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "          layer.trainable = False\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3W4Fl-0ZST-"
   },
   "outputs": [],
   "source": [
    "exec('model = ' + selected_model + '()')\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(learning_rate, decay = learning_rate * 0.1),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqgKvvNuzVWj"
   },
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### &ensp; *4.3 Model Fit* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2561599,
     "status": "ok",
     "timestamp": 1621525773176,
     "user": {
      "displayName": "He Did Nothing Wrong",
      "photoUrl": "",
      "userId": "14044224695877776076"
     },
     "user_tz": -180
    },
    "id": "20wQdiVldBkI",
    "outputId": "d6dcdb1d-73ff-43c2-a679-d9b835d17ed3"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train,\n",
    "                 validation_data = val,\n",
    "                 epochs = epochs,\n",
    "                 steps_per_epoch = len(x_train) // batch_size,\n",
    "                 callbacks=[checkpoint_acc, checkpoint_loss, lr_reducer, lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYdmQVatH_e4"
   },
   "outputs": [],
   "source": [
    "# Variable to mark whether the model is Fine Tuned in exported CSV name\n",
    "fine_t = ''\n",
    "\n",
    "# Load model function (Choose between Loss and Accuracy)\n",
    "def load_model(num):\n",
    "  if num == 0:\n",
    "    model.load_weights(path_c + 'Loss_CP - ' + selected_model + fine_t + '.h5')\n",
    "  else:\n",
    "    model.load_weights(path_c + 'Acc_CP - ' + selected_model + fine_t + '.h5')\n",
    "\n",
    "load_model(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9214,
     "status": "ok",
     "timestamp": 1621528605125,
     "user": {
      "displayName": "He Did Nothing Wrong",
      "photoUrl": "",
      "userId": "14044224695877776076"
     },
     "user_tz": -180
    },
    "id": "QqiIls58RhPi",
    "outputId": "b45e5dee-3fc7-4dc0-fee2-b2a01e407033"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbMWf0QFHmq-"
   },
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### &ensp; *4.4 Fine Tuning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OL_0_lx9Ht5Y"
   },
   "outputs": [],
   "source": [
    "# Unfreeze transfered model layers, besides batch normalization ones\n",
    "for layer in model.layers[unfreeze_layers:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(learning_rate*0.01),\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2767016,
     "status": "ok",
     "timestamp": 1621528583242,
     "user": {
      "displayName": "He Did Nothing Wrong",
      "photoUrl": "",
      "userId": "14044224695877776076"
     },
     "user_tz": -180
    },
    "id": "aR1sBlAgwWW6",
    "outputId": "9bc208de-3344-4769-e66f-592c46fe32c6"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train,\n",
    "                    validation_data = val,\n",
    "                    epochs = fine_tune_epochs,\n",
    "                    steps_per_epoch = len(x_train) // batch_size,\n",
    "                    callbacks = [checkpoint_acc_ft, checkpoint_loss_ft, lr_reducer,lr_scheduler, early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NLlnRYmIkr_"
   },
   "outputs": [],
   "source": [
    "# Changed to fine tuned\n",
    "fine_t = ' FT'\n",
    "\n",
    "# Loead weights that achieved the highest accuracy\n",
    "load_model(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRLXEVJIzeHr"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcsIDDeR8P2l"
   },
   "source": [
    "<a name=\"5.1\"></a>\n",
    "### &ensp; *5.1 Test Images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGEXPqE7APov"
   },
   "outputs": [],
   "source": [
    "# Read Test Images\n",
    "test_name = []\n",
    "x_test = []\n",
    "for img in os.listdir(test_path):\n",
    "  img_array = cv2.imread(os.path.join(test_path, img))\n",
    "  res_array = cv2.resize(img_array, (img_size, img_size))\n",
    "  test_name.append(img)\n",
    "  x_test.append(res_array)\n",
    "\n",
    "x_test = np.array(x_test).reshape(-1, img_size, img_size, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHSkLL1b8Utj"
   },
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### &ensp; *5.2 Export Predictions* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24653,
     "status": "ok",
     "timestamp": 1621528666297,
     "user": {
      "displayName": "He Did Nothing Wrong",
      "photoUrl": "",
      "userId": "14044224695877776076"
     },
     "user_tz": -180
    },
    "id": "647kSdIUnAfm",
    "outputId": "d5665870-f610-4b25-e45b-7b2302bfac5c"
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "result =  model.predict(x_test)\n",
    "softmax = model.predict(x_test, verbose=1)\n",
    "\n",
    "# get predict label\n",
    "predict_label = np.argmax(result,axis=-1)\n",
    "\n",
    "# Export CSV\n",
    "Submission = pd.DataFrame(list(zip(test_name, predict_label)), columns = [colx, coly])\n",
    "Submission.to_csv(sub + selected_model + ' - (' + str(img_size) + ',' + str(epochs) + ',' + str(batch_size) + ',' + str(learning_rate) + ',' + str(fine_tune_epochs) + ',' + str(unfreeze_layers) + ')' + fine_t + '.csv', index = False)\n",
    "\n",
    "Softmax = pd.DataFrame(softmax)\n",
    "Softmax.to_csv(sub + 'Softmax - ' + selected_model + ' - (' + str(img_size) + ',' + str(epochs) + ',' + str(batch_size) + ',' + str(learning_rate) + ',' + str(fine_tune_epochs) + ',' + str(unfreeze_layers) + ')' + fine_t + '.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGC99QBZ4ZUA"
   },
   "source": [
    "<a name=\"5.3\"></a>\n",
    "### &ensp; *5.3 Voting Ensemble*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAj_lxBdEBtj"
   },
   "outputs": [],
   "source": [
    "# Soft Voting\n",
    "soft = os.listdir(soft_path)\n",
    "\n",
    "# Create an array of zeros, with 3 columns, for each class, and columns equal\n",
    "# to the number of test images.\n",
    "# Read the CSVs from the path and sums the softmaxes values of all CSVs\n",
    "soft_predictions = np.zeros([len(test_name), num_cl])\n",
    "for soft_csv in soft:\n",
    "  predictions = pd.read_csv(soft_path+'/'+soft_csv, dtype=float)\n",
    "  soft_predictions = np.sum([soft_predictions, predictions], axis=0)\n",
    "\n",
    "# Returns the index of the biggest value by row. Which is equal to the class.\n",
    "soft_final = np.argmax(soft_predictions, axis=1)\n",
    "\n",
    "results = pd.DataFrame(list(zip(test_name, soft_final)),\n",
    "                       columns = [colx, coly])\n",
    "results.to_csv(soft_single, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPc8zi+IKEPEYnx14hGQtVo",
   "collapsed_sections": [],
   "name": "Effi x6 - Local.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
